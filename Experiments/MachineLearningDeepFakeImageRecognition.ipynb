{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe66a015-09eb-41cc-95ee-1f9b6bd8112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"TBB\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "929a9986-adf4-4005-a7d0-2c8d4a1092d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = ['lbp', 'hog', 'color', 'gabor'] #Added all features for baseline.\n",
    "use_pca = True #This way we can enable/disable PCA.\n",
    "pca_variance_threshold = 0.95 #Keep components explaining 95% of variance.\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=1),\n",
    "    'SVM': SVC(random_state=1, probability=True),  #Added probability=True for better predictions.\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f322bd7-8fd4-42c5-b11b-62b84f882c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions help set a loop/pipeline for experimenting with different features/models.\n",
    "def load_and_combine_features(real_path, fake_path, features_to_use):\n",
    "    \"\"\"Load features but keep them separate (don't concatenate yet)\"\"\"\n",
    "    real_data = np.load(real_path)\n",
    "    fake_data = np.load(fake_path)\n",
    "\n",
    "    print(f\"  Feature dimensions in {real_path}:\")\n",
    "    for feature in features_to_use:\n",
    "        print(f\"    {feature}: {real_data[feature].shape[1]} features\")\n",
    "    \n",
    "    #Store features separately as a dictionary.\n",
    "    feature_dict = {}\n",
    "    for feature in features_to_use:\n",
    "        real_feat = real_data[feature]\n",
    "        fake_feat = fake_data[feature]\n",
    "        #Combine real and fake for this feature type.\n",
    "        feature_dict[feature] = np.vstack([real_feat, fake_feat])\n",
    "    \n",
    "    #Create labels.\n",
    "    y = np.hstack([np.ones(len(real_data[features_to_use[0]])),np.zeros(len(fake_data[features_to_use[0]]))])\n",
    "    \n",
    "    return feature_dict, y\n",
    "\n",
    "def print_metrics(y_true, y_pred, y_pred_proba, dataset_name, model_name, inference_time=None):\n",
    "    #We choose the following evaluation metrics.\n",
    "    Acc = accuracy_score(y_true, y_pred)\n",
    "    F1 = f1_score(y_true, y_pred)\n",
    "    Precision = precision_score(y_true, y_pred)\n",
    "    Recall = recall_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    #Calculate ROC-AUC.\n",
    "    if y_pred_proba is not None:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    #Extract confusion matrix values.\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(f\"\\n{model_name} - {dataset_name} Results:\")\n",
    "    if inference_time is not None:\n",
    "        print(f\" Inference Time: {inference_time:.4f} seconds\")\n",
    "    print(f\" Accuracy: {Acc:.4f}\")\n",
    "    print(f\" Precision: {Precision:.4f}\")\n",
    "    print(f\" Recall: {Recall:.4f}\")\n",
    "    print(f\" F1 Score: {F1:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\" ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\" Confusion Matrix:\")\n",
    "    print(f\"   {cm}\")\n",
    "\n",
    "    print(f\"\\n CONFUSION MATRIX BREAKDOWN:\")\n",
    "    print(f\"  True Negatives  (TN): {tn:4d} - Correctly identified as Fake\")\n",
    "    print(f\"  False Positives (FP): {fp:4d} - Fake wrongly identified as Real\")\n",
    "    print(f\"  False Negatives (FN): {fn:4d} - Real wrongly identified as Fake\")\n",
    "    print(f\"  True Positives  (TP): {tp:4d} - Correctly identified as Real\")\n",
    "    \n",
    "    #Detailed Error analysis.\n",
    "    total_errors = fp + fn\n",
    "    total_samples = len(y_true)\n",
    "    print(f\"\\n ERROR ANALYSIS:\")\n",
    "    print(f\"  Total Errors: {total_errors}/{total_samples} ({total_errors/total_samples*100:.2f}%)\")\n",
    "    print(f\"  False Positives: {fp} ({fp/total_samples*100:.2f}%) - Fake images classified as Real\")\n",
    "    print(f\"  False Negatives: {fn} ({fn/total_samples*100:.2f}%) - Real images classified as Fake\")\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90c54c57-b6b3-4d53-871e-1c1caf352003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Train ---\n",
      "  Feature dimensions in train_real_all_features.npz:\n",
      "    lbp: 59 features\n",
      "    hog: 1764 features\n",
      "    color: 6 features\n",
      "    gabor: 8 features\n",
      "\n",
      "--- Validation ---\n",
      "  Feature dimensions in valid_real_all_features.npz:\n",
      "    lbp: 59 features\n",
      "    hog: 1764 features\n",
      "    color: 6 features\n",
      "    gabor: 8 features\n",
      "\n",
      "--- Test ---\n",
      "  Feature dimensions in test_real_all_features.npz:\n",
      "    lbp: 59 features\n",
      "    hog: 1764 features\n",
      "    color: 6 features\n",
      "    gabor: 8 features\n",
      "\n",
      "Train set: (10000, 1837)\n",
      "Valid set: (2000, 1837)\n",
      "Test set: (2000, 1837)\n",
      "Total features used: 1837\n",
      "\n",
      "--- Class Balance ---\n",
      "Train - Real: 5000, Fake: 5000\n",
      "Valid - Real: 1000, Fake: 1000\n",
      "Test  - Real: 1000, Fake: 1000\n"
     ]
    }
   ],
   "source": [
    "#Time to load the data.\n",
    "print(\"\\n--- Train ---\")\n",
    "X_train_dict, y_train = load_and_combine_features(\n",
    "    'train_real_all_features.npz',\n",
    "    'train_fake_all_features.npz',\n",
    "    features_to_use\n",
    ")\n",
    "print(\"\\n--- Validation ---\")\n",
    "X_valid_dict, y_valid = load_and_combine_features(\n",
    "    'valid_real_all_features.npz',\n",
    "    'valid_fake_all_features.npz',\n",
    "    features_to_use\n",
    ")\n",
    "print(\"\\n--- Test ---\")\n",
    "X_test_dict, y_test = load_and_combine_features(\n",
    "    'test_real_all_features.npz',\n",
    "    'test_fake_all_features.npz',\n",
    "    features_to_use\n",
    ")\n",
    "\n",
    "#Calculate total features.\n",
    "total_features = sum(X_train_dict[feat].shape[1] for feat in features_to_use)\n",
    "num_samples_train = X_train_dict[features_to_use[0]].shape[0]\n",
    "num_samples_valid = X_valid_dict[features_to_use[0]].shape[0]\n",
    "num_samples_test = X_test_dict[features_to_use[0]].shape[0]\n",
    "\n",
    "print(f\"\\nTrain set: ({num_samples_train}, {total_features})\")\n",
    "print(f\"Valid set: ({num_samples_valid}, {total_features})\")\n",
    "print(f\"Test set: ({num_samples_test}, {total_features})\")\n",
    "print(f\"Total features used: {total_features}\")\n",
    "\n",
    "print(\"\\n--- Class Balance ---\")\n",
    "print(f\"Train - Real: {np.sum(y_train == 0)}, Fake: {np.sum(y_train == 1)}\")\n",
    "print(f\"Valid - Real: {np.sum(y_valid == 0)}, Fake: {np.sum(y_valid == 1)}\")\n",
    "print(f\"Test  - Real: {np.sum(y_test == 0)}, Fake: {np.sum(y_test == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f34d4b5-5725-4eb5-811d-e0b18acc9d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaling features individually\n",
      "  Scaling lbp\n",
      "  Scaling hog\n",
      "  Scaling color\n",
      "  Scaling gabor\n",
      "\n",
      "Final concatenated shape: (10000, 1837)\n",
      "Scalers saved to 'scalers_dict.joblib'\n"
     ]
    }
   ],
   "source": [
    "#Scaling all features individually (before PCA).\n",
    "print(\"\\nScaling features individually\")\n",
    "\n",
    "#Dictionary to store scalers for each feature type.\n",
    "scalers = {}\n",
    "X_train_scaled_dict = {}\n",
    "X_valid_scaled_dict = {}\n",
    "X_test_scaled_dict = {}\n",
    "\n",
    "#Scale each feature type separately.\n",
    "for feature in features_to_use:\n",
    "    print(f\"  Scaling {feature}\")\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    #Fit on train, transform all sets.\n",
    "    X_train_scaled_dict[feature] = scaler.fit_transform(X_train_dict[feature])\n",
    "    X_valid_scaled_dict[feature] = scaler.transform(X_valid_dict[feature])\n",
    "    X_test_scaled_dict[feature] = scaler.transform(X_test_dict[feature])\n",
    "    \n",
    "    #Store scaler for this feature.\n",
    "    scalers[feature] = scaler\n",
    "\n",
    "#Now concatenate all scaled features.\n",
    "X_train_scaled = np.concatenate([X_train_scaled_dict[f] for f in features_to_use], axis=1)\n",
    "X_valid_scaled = np.concatenate([X_valid_scaled_dict[f] for f in features_to_use], axis=1)\n",
    "X_test_scaled = np.concatenate([X_test_scaled_dict[f] for f in features_to_use], axis=1)\n",
    "\n",
    "print(f\"\\nFinal concatenated shape: {X_train_scaled.shape}\")\n",
    "\n",
    "#Save scalers.\n",
    "joblib.dump(scalers, 'scalers_dict.joblib')\n",
    "print(\"Scalers saved to 'scalers_dict.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bad2ddfb-65c4-4520-b731-b80ec03f1d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA Results:\n",
      " Original features: 1837\n",
      " Components retained: 480\n",
      " Explained variance: 0.9501 (95.01%)\n",
      " Dimensionality reduction: 1837 --> 480\n",
      "PCA saved to 'pca.joblib'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#Apply PCA.\n",
    "if use_pca:\n",
    "    #We fit PCA on training data only.\n",
    "    pca = PCA(n_components=pca_variance_threshold, random_state=1)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "    #We transform validation and test using the fitted PCA.\n",
    "    X_valid_pca = pca.transform(X_valid_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    n_components = pca.n_components_\n",
    "    explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"\\nPCA Results:\")\n",
    "    print(f\" Original features: {X_train_scaled.shape[1]}\")\n",
    "    print(f\" Components retained: {n_components}\")\n",
    "    print(f\" Explained variance: {explained_var:.4f} ({explained_var*100:.2f}%)\")\n",
    "    print(f\" Dimensionality reduction: {X_train_scaled.shape[1]} --> {n_components}\")\n",
    "\n",
    "    joblib.dump(pca, 'pca.joblib')\n",
    "    print(\"PCA saved to 'pca.joblib'\")\n",
    "\n",
    "    #Use PCA-transformed data for training.\n",
    "    X_train_final = X_train_pca\n",
    "    X_valid_final = X_valid_pca\n",
    "    X_test_final = X_test_pca\n",
    "\n",
    "else:\n",
    "    print(\"\\nPCA Disabled\")\n",
    "    X_train_final = X_train_scaled\n",
    "    X_valid_final = X_valid_scaled\n",
    "    X_test_final = X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_training",
   "metadata": {},
   "source": [
    "## Baseline Training - All Models\n",
    "Train all models with default hyperparameters and evaluate on validation set for hyperparameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f181380-1ba9-4cc3-bb02-a2db131eda76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+++++++++++++++++++++++++\n",
      "Training Random Forest\n",
      "\n",
      "Training completed in 58.54 seconds\n",
      "\n",
      "Random Forest - Train Results:\n",
      " Accuracy: 1.0000\n",
      " Precision: 1.0000\n",
      " Recall: 1.0000\n",
      " F1 Score: 1.0000\n",
      " ROC-AUC: 1.0000\n",
      " Confusion Matrix:\n",
      "   [[5000    0]\n",
      " [   0 5000]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN): 5000 - Correctly identified as Fake\n",
      "  False Positives (FP):    0 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):    0 - Real wrongly identified as Fake\n",
      "  True Positives  (TP): 5000 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 0/10000 (0.00%)\n",
      "  False Positives: 0 (0.00%) - Fake images classified as Real\n",
      "  False Negatives: 0 (0.00%) - Real images classified as Fake\n",
      "\n",
      "Random Forest - Validation Results:\n",
      " Inference Time: 0.0782 seconds\n",
      " Accuracy: 0.7145\n",
      " Precision: 0.7173\n",
      " Recall: 0.7080\n",
      " F1 Score: 0.7126\n",
      " ROC-AUC: 0.7924\n",
      " Confusion Matrix:\n",
      "   [[721 279]\n",
      " [292 708]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN):  721 - Correctly identified as Fake\n",
      "  False Positives (FP):  279 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  292 - Real wrongly identified as Fake\n",
      "  True Positives  (TP):  708 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 571/2000 (28.55%)\n",
      "  False Positives: 279 (13.95%) - Fake images classified as Real\n",
      "  False Negatives: 292 (14.60%) - Real images classified as Fake\n",
      "\n",
      "Model saved to 'random_forest_model.joblib'\n",
      "\n",
      "+++++++++++++++++++++++++\n",
      "Training Gradient Boosting\n",
      "\n",
      "Training completed in 357.73 seconds\n",
      "\n",
      "Gradient Boosting - Train Results:\n",
      " Accuracy: 0.8331\n",
      " Precision: 0.8426\n",
      " Recall: 0.8192\n",
      " F1 Score: 0.8307\n",
      " ROC-AUC: 0.9133\n",
      " Confusion Matrix:\n",
      "   [[4235  765]\n",
      " [ 904 4096]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN): 4235 - Correctly identified as Fake\n",
      "  False Positives (FP):  765 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  904 - Real wrongly identified as Fake\n",
      "  True Positives  (TP): 4096 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 1669/10000 (16.69%)\n",
      "  False Positives: 765 (7.65%) - Fake images classified as Real\n",
      "  False Negatives: 904 (9.04%) - Real images classified as Fake\n",
      "\n",
      "Gradient Boosting - Validation Results:\n",
      " Inference Time: 0.0090 seconds\n",
      " Accuracy: 0.7325\n",
      " Precision: 0.7530\n",
      " Recall: 0.6920\n",
      " F1 Score: 0.7212\n",
      " ROC-AUC: 0.8063\n",
      " Confusion Matrix:\n",
      "   [[773 227]\n",
      " [308 692]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN):  773 - Correctly identified as Fake\n",
      "  False Positives (FP):  227 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  308 - Real wrongly identified as Fake\n",
      "  True Positives  (TP):  692 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 535/2000 (26.75%)\n",
      "  False Positives: 227 (11.35%) - Fake images classified as Real\n",
      "  False Negatives: 308 (15.40%) - Real images classified as Fake\n",
      "\n",
      "Model saved to 'gradient_boosting_model.joblib'\n",
      "\n",
      "+++++++++++++++++++++++++\n",
      "Training SVM\n",
      "\n",
      "Training completed in 239.07 seconds\n",
      "\n",
      "SVM - Train Results:\n",
      " Accuracy: 0.9561\n",
      " Precision: 0.9569\n",
      " Recall: 0.9552\n",
      " F1 Score: 0.9561\n",
      " ROC-AUC: 0.9914\n",
      " Confusion Matrix:\n",
      "   [[4785  215]\n",
      " [ 224 4776]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN): 4785 - Correctly identified as Fake\n",
      "  False Positives (FP):  215 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  224 - Real wrongly identified as Fake\n",
      "  True Positives  (TP): 4776 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 439/10000 (4.39%)\n",
      "  False Positives: 215 (2.15%) - Fake images classified as Real\n",
      "  False Negatives: 224 (2.24%) - Real images classified as Fake\n",
      "\n",
      "SVM - Validation Results:\n",
      " Inference Time: 10.7706 seconds\n",
      " Accuracy: 0.8265\n",
      " Precision: 0.8369\n",
      " Recall: 0.8110\n",
      " F1 Score: 0.8238\n",
      " ROC-AUC: 0.9010\n",
      " Confusion Matrix:\n",
      "   [[842 158]\n",
      " [189 811]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN):  842 - Correctly identified as Fake\n",
      "  False Positives (FP):  158 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  189 - Real wrongly identified as Fake\n",
      "  True Positives  (TP):  811 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 347/2000 (17.35%)\n",
      "  False Positives: 158 (7.90%) - Fake images classified as Real\n",
      "  False Negatives: 189 (9.45%) - Real images classified as Fake\n",
      "\n",
      "Model saved to 'svm_model.joblib'\n",
      "\n",
      "+++++++++++++++++++++++++\n",
      "Training KNN\n",
      "\n",
      "Training completed in 0.03 seconds\n",
      "\n",
      "KNN - Train Results:\n",
      " Accuracy: 1.0000\n",
      " Precision: 1.0000\n",
      " Recall: 1.0000\n",
      " F1 Score: 1.0000\n",
      " ROC-AUC: 1.0000\n",
      " Confusion Matrix:\n",
      "   [[5000    0]\n",
      " [   0 5000]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN): 5000 - Correctly identified as Fake\n",
      "  False Positives (FP):    0 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):    0 - Real wrongly identified as Fake\n",
      "  True Positives  (TP): 5000 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 0/10000 (0.00%)\n",
      "  False Positives: 0 (0.00%) - Fake images classified as Real\n",
      "  False Negatives: 0 (0.00%) - Real images classified as Fake\n",
      "\n",
      "KNN - Validation Results:\n",
      " Inference Time: 0.5547 seconds\n",
      " Accuracy: 0.6090\n",
      " Precision: 0.8225\n",
      " Recall: 0.2780\n",
      " F1 Score: 0.4155\n",
      " ROC-AUC: 0.6090\n",
      " Confusion Matrix:\n",
      "   [[940  60]\n",
      " [722 278]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN):  940 - Correctly identified as Fake\n",
      "  False Positives (FP):   60 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  722 - Real wrongly identified as Fake\n",
      "  True Positives  (TP):  278 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 782/2000 (39.10%)\n",
      "  False Positives: 60 (3.00%) - Fake images classified as Real\n",
      "  False Negatives: 722 (36.10%) - Real images classified as Fake\n",
      "\n",
      "Model saved to 'knn_model.joblib'\n",
      "\n",
      "=========================\n",
      "VALIDATION PHASE COMPLETE\n",
      "=========================\n",
      "\n",
      "Validation Results Summary:\n",
      "Model                Accuracy   F1-Score   ROC-AUC    Train(s)   Infer(s)  \n",
      "-----------------------------------\n",
      "Random Forest        0.7145     0.7126     0.7924     58.54      0.0782    \n",
      "Gradient Boosting    0.7325     0.7212     0.8063     357.73     0.0090    \n",
      "SVM                  0.8265     0.8238     0.9010     239.07     10.7706   \n",
      "KNN                  0.6090     0.4155     0.6090     0.03       0.5547    \n",
      "\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "#Time to train and evaluate all models on validation set.\n",
    "trained_models = {}\n",
    "roc_data = {} #Store ROC curve data for plotting.\n",
    "validation_results = {}  #Store validation metrics for comparison.\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'+'*25}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "\n",
    "    #Start timing for training.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Training.\n",
    "    model.fit(X_train_final, y_train)\n",
    "    \n",
    "    #End timing for training.\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    #Predicting on train set (for overfitting check).\n",
    "    y_train_pred = model.predict(X_train_final)\n",
    "    \n",
    "    #Predicting on validation set (along with inference time tracking).\n",
    "    inference_start = time.time()\n",
    "    y_valid_pred = model.predict(X_valid_final)\n",
    "    inference_time = time.time() - inference_start\n",
    "    \n",
    "    # Check if model supports predict_proba to get probability predictions for ROC-AUC.\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_train_proba = model.predict_proba(X_train_final)[:, 1]\n",
    "        y_valid_proba = model.predict_proba(X_valid_final)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):  #For SVM.\n",
    "        y_train_proba = model.decision_function(X_train_final)\n",
    "        y_valid_proba = model.decision_function(X_valid_final)\n",
    "    else:\n",
    "        y_train_proba = None\n",
    "        y_valid_proba = None\n",
    "\n",
    "    #Printing metrics for train and validation.\n",
    "    print_metrics(y_train, y_train_pred, y_train_proba, \"Train\", model_name)\n",
    "    valid_auc = print_metrics(y_valid, y_valid_pred, y_valid_proba, \"Validation\", model_name, inference_time)\n",
    "    \n",
    "    #Store validation results for comparison.\n",
    "    validation_results[model_name] = {\n",
    "        'accuracy': accuracy_score(y_valid, y_valid_pred),\n",
    "        'f1_score': f1_score(y_valid, y_valid_pred),\n",
    "        'roc_auc': valid_auc,\n",
    "        'training_time': training_time,\n",
    "        'inference_time': inference_time\n",
    "    }\n",
    "    \n",
    "    #Store ROC curve data for validation set.\n",
    "    if y_valid_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_valid, y_valid_proba)\n",
    "        roc_data[model_name] = {\n",
    "            'fpr': fpr, \n",
    "            'tpr': tpr, \n",
    "            'auc': valid_auc,\n",
    "            'training_time': training_time,\n",
    "            'inference_time': inference_time\n",
    "        }\n",
    "\n",
    "    #Saving models.\n",
    "    model_filename = f\"{model_name.lower().replace(' ', '_')}_model.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"\\nModel saved to '{model_filename}'\")\n",
    "\n",
    "    #Storing in dictionary.\n",
    "    trained_models[model_name] = model\n",
    "\n",
    "print(\"\\n\" + \"=\"*25)\n",
    "print(\"VALIDATION PHASE COMPLETE\")\n",
    "print(\"=\"*25)\n",
    "print(\"\\nValidation Results Summary:\")\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'F1-Score':<10} {'ROC-AUC':<10} {'Train(s)':<10} {'Infer(s)':<10}\")\n",
    "print(\"-\"*35)\n",
    "for model_name, results in validation_results.items():\n",
    "    print(f\"{model_name:<20} {results['accuracy']:<10.4f} {results['f1_score']:<10.4f} \"\n",
    "          f\"{results['roc_auc']:<10.4f} {results['training_time']:<10.2f} {results['inference_time']:<10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca_experimentation",
   "metadata": {},
   "source": [
    "## PCA Variance Threshold Experimentation\n",
    "Test different PCA variance thresholds to find the optimal balance between dimensionality reduction and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "pca_experiment_block",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "PCA VARIANCE THRESHOLD EXPERIMENTATION\n",
      "Testing model: SVM\n",
      "====================\n",
      "\n",
      "\n",
      "*************************\n",
      "Testing PCA variance threshold: 0.8 (80%)\n",
      "*************************\n",
      "\n",
      "PCA Info:\n",
      "  Components retained: 207\n",
      "  Explained variance: 0.8005 (80.05%)\n",
      "  Dimensionality: 1837 --> 207\n",
      "  Reduction: 88.7%\n",
      "\n",
      "Validation Performance:\n",
      "  Accuracy:  0.8235\n",
      "  Precision: 0.8373\n",
      "  Recall:    0.8030\n",
      "  F1-Score:  0.8198\n",
      "  ROC-AUC:   0.8992\n",
      "  Training time:   114.6095s\n",
      "  Inference time:  4.6556s\n",
      "\n",
      "*************************\n",
      "Testing PCA variance threshold: 0.85 (85%)\n",
      "*************************\n",
      "\n",
      "PCA Info:\n",
      "  Components retained: 267\n",
      "  Explained variance: 0.8504 (85.04%)\n",
      "  Dimensionality: 1837 --> 267\n",
      "  Reduction: 85.5%\n",
      "\n",
      "Validation Performance:\n",
      "  Accuracy:  0.8230\n",
      "  Precision: 0.8393\n",
      "  Recall:    0.7990\n",
      "  F1-Score:  0.8186\n",
      "  ROC-AUC:   0.9002\n",
      "  Training time:   141.1943s\n",
      "  Inference time:  5.5716s\n",
      "\n",
      "*************************\n",
      "Testing PCA variance threshold: 0.9 (90%)\n",
      "*************************\n",
      "\n",
      "PCA Info:\n",
      "  Components retained: 349\n",
      "  Explained variance: 0.9002 (90.02%)\n",
      "  Dimensionality: 1837 --> 349\n",
      "  Reduction: 81.0%\n",
      "\n",
      "Validation Performance:\n",
      "  Accuracy:  0.8250\n",
      "  Precision: 0.8385\n",
      "  Recall:    0.8050\n",
      "  F1-Score:  0.8214\n",
      "  ROC-AUC:   0.9023\n",
      "  Training time:   184.8515s\n",
      "  Inference time:  7.4156s\n",
      "\n",
      "*************************\n",
      "Testing PCA variance threshold: 0.95 (95%)\n",
      "*************************\n",
      "\n",
      "PCA Info:\n",
      "  Components retained: 480\n",
      "  Explained variance: 0.9501 (95.01%)\n",
      "  Dimensionality: 1837 --> 480\n",
      "  Reduction: 73.9%\n",
      "\n",
      "Validation Performance:\n",
      "  Accuracy:  0.8265\n",
      "  Precision: 0.8369\n",
      "  Recall:    0.8110\n",
      "  F1-Score:  0.8238\n",
      "  ROC-AUC:   0.9010\n",
      "  Training time:   237.2855s\n",
      "  Inference time:  9.7486s\n",
      "\n",
      "*************************\n",
      "Testing PCA variance threshold: 0.99 (99%)\n",
      "*************************\n",
      "\n",
      "PCA Info:\n",
      "  Components retained: 875\n",
      "  Explained variance: 0.9900 (99.00%)\n",
      "  Dimensionality: 1837 --> 875\n",
      "  Reduction: 52.4%\n",
      "\n",
      "Validation Performance:\n",
      "  Accuracy:  0.8235\n",
      "  Precision: 0.8325\n",
      "  Recall:    0.8100\n",
      "  F1-Score:  0.8211\n",
      "  ROC-AUC:   0.9035\n",
      "  Training time:   412.0747s\n",
      "  Inference time:  18.6363s\n",
      "\n",
      "=========================\n",
      "PCA VARIANCE THRESHOLD COMPARISON - SVM\n",
      "=========================\n",
      "Threshold    Components   Accuracy   F1-Score   ROC-AUC    Train(s)   Infer(s)  \n",
      "-------------------------\n",
      "0.80         207          0.8235     0.8198     0.8992     114.6095   4.6556    \n",
      "0.85         267          0.8230     0.8186     0.9002     141.1943   5.5716    \n",
      "0.90         349          0.8250     0.8214     0.9023     184.8515   7.4156    \n",
      "0.95         480          0.8265     0.8238     0.9010     237.2855   9.7486    \n",
      "0.99         875          0.8235     0.8211     0.9035     412.0747   18.6363   \n",
      "\n",
      "=========================\n",
      "OPTIMAL THRESHOLDS:\n",
      "  Best Accuracy:  0.95 (Acc: 0.8265, Components: 480)\n",
      "  Best F1-Score:  0.95 (F1: 0.8238, Components: 480)\n",
      "  Best ROC-AUC:   0.99 (AUC: 0.9035, Components: 875)\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "#PCA Variance Threshold Experimentation.\n",
    "\n",
    "test_model = 'SVM'\n",
    "variance_thresholds = [0.80, 0.85, 0.90, 0.95, 0.99]\n",
    "\n",
    "print(f\"\\n{'='*20}\")\n",
    "print(f\"PCA VARIANCE THRESHOLD EXPERIMENTATION\")\n",
    "print(f\"Testing model: {test_model}\")\n",
    "print(f\"{'='*20}\\n\")\n",
    "\n",
    "pca_results = []\n",
    "\n",
    "for threshold in variance_thresholds:\n",
    "    print(f\"\\n{'*'*25}\")\n",
    "    print(f\"Testing PCA variance threshold: {threshold} ({threshold*100:.0f}%)\")\n",
    "    print(f\"{'*'*25}\")\n",
    "    \n",
    "    #Apply PCA with current threshold.\n",
    "    pca_temp = PCA(n_components=threshold, random_state=1)\n",
    "    X_train_pca_temp = pca_temp.fit_transform(X_train_scaled)\n",
    "    X_valid_pca_temp = pca_temp.transform(X_valid_scaled)\n",
    "    \n",
    "    n_components = pca_temp.n_components_\n",
    "    explained_var = np.sum(pca_temp.explained_variance_ratio_)\n",
    "    \n",
    "    print(f\"\\nPCA Info:\")\n",
    "    print(f\"  Components retained: {n_components}\")\n",
    "    print(f\"  Explained variance: {explained_var:.4f} ({explained_var*100:.2f}%)\")\n",
    "    print(f\"  Dimensionality: {X_train_scaled.shape[1]} --> {n_components}\")\n",
    "    print(f\"  Reduction: {(1 - n_components/X_train_scaled.shape[1])*100:.1f}%\")\n",
    "    \n",
    "    #Train model with this PCA configuration.\n",
    "    model_temp = models[test_model].__class__(**models[test_model].get_params())\n",
    "    \n",
    "    #Training.\n",
    "    train_start = time.time()\n",
    "    model_temp.fit(X_train_pca_temp, y_train)\n",
    "    train_time = time.time() - train_start\n",
    "    \n",
    "    # Validation prediction with inference time\n",
    "    infer_start = time.time()\n",
    "    y_valid_pred_temp = model_temp.predict(X_valid_pca_temp)\n",
    "    infer_time = time.time() - infer_start\n",
    "    \n",
    "    #Get probabilities.\n",
    "    if hasattr(model_temp, \"predict_proba\"):\n",
    "        y_valid_proba_temp = model_temp.predict_proba(X_valid_pca_temp)[:, 1]\n",
    "    elif hasattr(model_temp, \"decision_function\"):\n",
    "        y_valid_proba_temp = model_temp.decision_function(X_valid_pca_temp)\n",
    "    else:\n",
    "        y_valid_proba_temp = None\n",
    "    \n",
    "    #Calculate metrics.\n",
    "    acc = accuracy_score(y_valid, y_valid_pred_temp)\n",
    "    f1 = f1_score(y_valid, y_valid_pred_temp)\n",
    "    precision = precision_score(y_valid, y_valid_pred_temp)\n",
    "    recall = recall_score(y_valid, y_valid_pred_temp)\n",
    "    \n",
    "    if y_valid_proba_temp is not None:\n",
    "        roc_auc = roc_auc_score(y_valid, y_valid_proba_temp)\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    print(f\"\\nValidation Performance:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"  Training time:   {train_time:.4f}s\")\n",
    "    print(f\"  Inference time:  {infer_time:.4f}s\")\n",
    "    \n",
    "    #Store results.\n",
    "    pca_results.append({\n",
    "        'threshold': threshold,\n",
    "        'n_components': n_components,\n",
    "        'explained_variance': explained_var,\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        'train_time': train_time,\n",
    "        'inference_time': infer_time\n",
    "    })\n",
    "\n",
    "#Summary table.\n",
    "print(f\"\\n{'='*25}\")\n",
    "print(f\"PCA VARIANCE THRESHOLD COMPARISON - {test_model}\")\n",
    "print(f\"{'='*25}\")\n",
    "print(f\"{'Threshold':<12} {'Components':<12} {'Accuracy':<10} {'F1-Score':<10} {'ROC-AUC':<10} {'Train(s)':<10} {'Infer(s)':<10}\")\n",
    "print(\"-\"*25)\n",
    "\n",
    "for result in pca_results:\n",
    "    print(f\"{result['threshold']:<12.2f} {result['n_components']:<12d} {result['accuracy']:<10.4f} \"\n",
    "          f\"{result['f1_score']:<10.4f} {result['roc_auc']:<10.4f} \"\n",
    "          f\"{result['train_time']:<10.4f} {result['inference_time']:<10.4f}\")\n",
    "\n",
    "#Find best threshold.\n",
    "best_by_accuracy = max(pca_results, key=lambda x: x['accuracy'])\n",
    "best_by_f1 = max(pca_results, key=lambda x: x['f1_score'])\n",
    "best_by_auc = max(pca_results, key=lambda x: x['roc_auc']) if pca_results[0]['roc_auc'] else None\n",
    "\n",
    "print(f\"\\n{'='*25}\")\n",
    "print(\"OPTIMAL THRESHOLDS:\")\n",
    "print(f\"  Best Accuracy:  {best_by_accuracy['threshold']:.2f} (Acc: {best_by_accuracy['accuracy']:.4f}, Components: {best_by_accuracy['n_components']})\")\n",
    "print(f\"  Best F1-Score:  {best_by_f1['threshold']:.2f} (F1: {best_by_f1['f1_score']:.4f}, Components: {best_by_f1['n_components']})\")\n",
    "if best_by_auc:\n",
    "    print(f\"  Best ROC-AUC:   {best_by_auc['threshold']:.2f} (AUC: {best_by_auc['roc_auc']:.4f}, Components: {best_by_auc['n_components']})\")\n",
    "print(f\"{'='*25}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual_finetuning",
   "metadata": {},
   "source": [
    "## Individual Model Fine-Tuning\n",
    "Fine-tune a specific model by adjusting its hyperparameters. Use validation set results to guide hyperparameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "individual_finetune_block",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "FINE-TUNING: SVM\n",
      "====================\n",
      "\n",
      "Model Configuration:\n",
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': True, 'random_state': 1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\n",
      "****************************************\n",
      "Training fine-tuned model...\n",
      "Training completed in 442.10 seconds\n",
      "\n",
      "Fine-tuned SVM - Train Results:\n",
      " Accuracy: 1.0000\n",
      " Precision: 1.0000\n",
      " Recall: 1.0000\n",
      " F1 Score: 1.0000\n",
      " ROC-AUC: 1.0000\n",
      " Confusion Matrix:\n",
      "   [[5000    0]\n",
      " [   0 5000]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN): 5000 - Correctly identified as Fake\n",
      "  False Positives (FP):    0 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):    0 - Real wrongly identified as Fake\n",
      "  True Positives  (TP): 5000 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 0/10000 (0.00%)\n",
      "  False Positives: 0 (0.00%) - Fake images classified as Real\n",
      "  False Negatives: 0 (0.00%) - Real images classified as Fake\n",
      "\n",
      "Fine-tuned SVM - Validation Results:\n",
      " Inference Time: 13.6995 seconds\n",
      " Accuracy: 0.8515\n",
      " Precision: 0.8681\n",
      " Recall: 0.8290\n",
      " F1 Score: 0.8481\n",
      " ROC-AUC: 0.9260\n",
      " Confusion Matrix:\n",
      "   [[874 126]\n",
      " [171 829]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN):  874 - Correctly identified as Fake\n",
      "  False Positives (FP):  126 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  171 - Real wrongly identified as Fake\n",
      "  True Positives  (TP):  829 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 297/2000 (14.85%)\n",
      "  False Positives: 126 (6.30%) - Fake images classified as Real\n",
      "  False Negatives: 171 (8.55%) - Real images classified as Fake\n",
      "\n",
      "=========================\n",
      "Comparison Wtih Baseline\n",
      "=========================\n",
      "\n",
      "Metric               Baseline        Fine-tuned      Change         \n",
      "-----------------------------------------------------------------\n",
      "Accuracy             0.8265          0.8515          +0.0250\n",
      "F1-Score             0.8238          0.8481          +0.0243\n",
      "ROC-AUC              0.9010          0.9260          +0.0250\n",
      "Training Time (s)    239.07          442.10          +203.04\n",
      "Inference Time (s)   10.7706         13.6995         +2.9288\n",
      "\n",
      "Fine-tuned model saved to 'svm_finetuned_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "#Individual Model Fine-Tuning.\n",
    "\n",
    "model_to_finetune = 'SVM'\n",
    "\n",
    "if model_to_finetune == 'SVM':\n",
    "    fine_tuned_model = SVC(\n",
    "        C=10,\n",
    "        kernel='rbf',\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "elif model_to_finetune == 'Random Forest':\n",
    "    fine_tuned_model = RandomForestClassifier(\n",
    "        n_estimators=350,\n",
    "        max_depth=50,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "elif model_to_finetune == 'Gradient Boosting':\n",
    "    fine_tuned_model = GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.2,\n",
    "        max_depth=5,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        subsample=1.0,\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "elif model_to_finetune == 'KNN':\n",
    "    fine_tuned_model = KNeighborsClassifier(\n",
    "        n_neighbors=1,\n",
    "        weights='uniform',\n",
    "        algorithm='auto',\n",
    "        metric='minkowski',\n",
    "        p=2\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'='*20}\")\n",
    "print(f\"FINE-TUNING: {model_to_finetune}\")\n",
    "print(f\"{'='*20}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(fine_tuned_model.get_params())\n",
    "\n",
    "#Training.\n",
    "print(f\"\\n{'*'*40}\")\n",
    "print(\"Training fine-tuned model...\")\n",
    "train_start = time.time()\n",
    "fine_tuned_model.fit(X_train_final, y_train)\n",
    "train_time = time.time() - train_start\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "#Predictions on train set.\n",
    "y_train_pred_ft = fine_tuned_model.predict(X_train_final)\n",
    "\n",
    "#Predictions on validation set with inference time.\n",
    "infer_start = time.time()\n",
    "y_valid_pred_ft = fine_tuned_model.predict(X_valid_final)\n",
    "infer_time = time.time() - infer_start\n",
    "\n",
    "#Get probabilities.\n",
    "if hasattr(fine_tuned_model, \"predict_proba\"):\n",
    "    y_train_proba_ft = fine_tuned_model.predict_proba(X_train_final)[:, 1]\n",
    "    y_valid_proba_ft = fine_tuned_model.predict_proba(X_valid_final)[:, 1]\n",
    "elif hasattr(fine_tuned_model, \"decision_function\"):\n",
    "    y_train_proba_ft = fine_tuned_model.decision_function(X_train_final)\n",
    "    y_valid_proba_ft = fine_tuned_model.decision_function(X_valid_final)\n",
    "else:\n",
    "    y_train_proba_ft = None\n",
    "    y_valid_proba_ft = None\n",
    "\n",
    "#Print metrics.\n",
    "print_metrics(y_train, y_train_pred_ft, y_train_proba_ft, \"Train\", f\"Fine-tuned {model_to_finetune}\")\n",
    "print_metrics(y_valid, y_valid_pred_ft, y_valid_proba_ft, \"Validation\", f\"Fine-tuned {model_to_finetune}\", infer_time)\n",
    "\n",
    "#Comparison with baseline.\n",
    "print(f\"\\n{'='*25}\")\n",
    "print(\"Comparison Wtih Baseline\")\n",
    "print(f\"{'='*25}\")\n",
    "\n",
    "baseline_acc = validation_results[model_to_finetune]['accuracy']\n",
    "baseline_f1 = validation_results[model_to_finetune]['f1_score']\n",
    "baseline_auc = validation_results[model_to_finetune]['roc_auc']\n",
    "baseline_train_time = validation_results[model_to_finetune]['training_time']\n",
    "baseline_infer_time = validation_results[model_to_finetune]['inference_time']\n",
    "\n",
    "finetuned_acc = accuracy_score(y_valid, y_valid_pred_ft)\n",
    "finetuned_f1 = f1_score(y_valid, y_valid_pred_ft)\n",
    "finetuned_auc = roc_auc_score(y_valid, y_valid_proba_ft) if y_valid_proba_ft is not None else None\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Baseline':<15} {'Fine-tuned':<15} {'Change':<15}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'Accuracy':<20} {baseline_acc:<15.4f} {finetuned_acc:<15.4f} {finetuned_acc - baseline_acc:+.4f}\")\n",
    "print(f\"{'F1-Score':<20} {baseline_f1:<15.4f} {finetuned_f1:<15.4f} {finetuned_f1 - baseline_f1:+.4f}\")\n",
    "if finetuned_auc:\n",
    "    print(f\"{'ROC-AUC':<20} {baseline_auc:<15.4f} {finetuned_auc:<15.4f} {finetuned_auc - baseline_auc:+.4f}\")\n",
    "print(f\"{'Training Time (s)':<20} {baseline_train_time:<15.2f} {train_time:<15.2f} {train_time - baseline_train_time:+.2f}\")\n",
    "print(f\"{'Inference Time (s)':<20} {baseline_infer_time:<15.4f} {infer_time:<15.4f} {infer_time - baseline_infer_time:+.4f}\")\n",
    "\n",
    "#Save fine-tuned model if performance improved.\n",
    "if finetuned_acc > baseline_acc or finetuned_f1 > baseline_f1:\n",
    "    model_filename = f\"{model_to_finetune.lower().replace(' ', '_')}_finetuned_model.joblib\"\n",
    "    joblib.dump(fine_tuned_model, model_filename)\n",
    "    print(f\"\\nFine-tuned model saved to '{model_filename}'\")\n",
    "else:\n",
    "    print(f\"\\nNo improvement detected.\")\n",
    "\n",
    "trained_models[model_to_finetune] = fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_test_evaluation",
   "metadata": {},
   "source": [
    "## Final Test Set Evaluation\n",
    "**IMPORTANT**: Run this cell only ONCE after you have completed all hyperparameter tuning and model selection.\n",
    "This evaluates the final selected model on the untouched test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "final_test_block",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########################\n",
      "  Final Test Set Evaluation\n",
      "  Selected Model: SVM\n",
      "\n",
      "Using baseline model: SVM\n",
      "\n",
      "********************\n",
      "Evaluating on test set\n",
      "\n",
      "SVM - Test Set (Final) Results:\n",
      " Inference Time: 11.6424 seconds\n",
      " Accuracy: 0.8480\n",
      " Precision: 0.8588\n",
      " Recall: 0.8330\n",
      " F1 Score: 0.8457\n",
      " ROC-AUC: 0.9276\n",
      " Confusion Matrix:\n",
      "   [[863 137]\n",
      " [167 833]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN):  863 - Correctly identified as Fake\n",
      "  False Positives (FP):  137 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  167 - Real wrongly identified as Fake\n",
      "  True Positives  (TP):  833 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 304/2000 (15.20%)\n",
      "  False Positives: 137 (6.85%) - Fake images classified as Real\n",
      "  False Negatives: 167 (8.35%) - Real images classified as Fake\n",
      "\n",
      "=========================\n",
      "FINAL MODEL PERFORMANCE SUMMARY\n",
      "=========================\n",
      "Model: SVM\n",
      "\n",
      "Dataset         Accuracy     F1-Score     ROC-AUC     \n",
      "-------------------------\n",
      "Validation      0.8265       0.8238       0.9010      \n",
      "Test (Final)    0.8480       0.8457       0.9276      \n",
      "\n",
      "=========================\n",
      "Generalization Gap (Validation → Test): +0.0215\n"
     ]
    }
   ],
   "source": [
    "#Final Test Set Evaluation.\n",
    "final_model_choice = 'SVM'\n",
    "\n",
    "print(f\"\\n{'#'*25}\")\n",
    "print(f\"  Final Test Set Evaluation\")\n",
    "print(f\"  Selected Model: {final_model_choice}\")\n",
    "\n",
    "#Load selected model.\n",
    "if final_model_choice == 'fine_tuned':\n",
    "    try:\n",
    "        final_model = fine_tuned_model\n",
    "        print(f\"\\nUsing fine-tuned model from previous cell\")\n",
    "    except NameError:\n",
    "        print(\"\\nERROR: Fine-tuned model not found. Please run the fine-tuning cell first.\")\n",
    "        raise\n",
    "else:\n",
    "    final_model = trained_models[final_model_choice]\n",
    "    print(f\"\\nUsing baseline model: {final_model_choice}\")\n",
    "\n",
    "#Test Set Prediction.\n",
    "print(f\"\\n{'*'*20}\")\n",
    "print(\"Evaluating on test set\")\n",
    "test_infer_start = time.time()\n",
    "y_test_pred = final_model.predict(X_test_final)\n",
    "test_infer_time = time.time() - test_infer_start\n",
    "\n",
    "#Get probabilities for test set.\n",
    "if hasattr(final_model, \"predict_proba\"):\n",
    "    y_test_proba = final_model.predict_proba(X_test_final)[:, 1]\n",
    "elif hasattr(final_model, \"decision_function\"):\n",
    "    y_test_proba = final_model.decision_function(X_test_final)\n",
    "else:\n",
    "    y_test_proba = None\n",
    "\n",
    "#Print final test results.\n",
    "test_auc = print_metrics(y_test, y_test_pred, y_test_proba, \"Test Set (Final)\", final_model_choice, test_infer_time)\n",
    "\n",
    "#Final summary.\n",
    "print(f\"\\n{'='*25}\")\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*25}\")\n",
    "print(f\"Model: {final_model_choice}\")\n",
    "print(f\"\\n{'Dataset':<15} {'Accuracy':<12} {'F1-Score':<12} {'ROC-AUC':<12}\")\n",
    "print(\"-\"*25)\n",
    "\n",
    "#Validation performance.\n",
    "if final_model_choice in validation_results:\n",
    "    val_results = validation_results[final_model_choice]\n",
    "    print(f\"{'Validation':<15} {val_results['accuracy']:<12.4f} {val_results['f1_score']:<12.4f} {val_results['roc_auc']:<12.4f}\")\n",
    "\n",
    "#Test performance.\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "print(f\"{'Test (Final)':<15} {test_acc:<12.4f} {test_f1:<12.4f} {test_auc:<12.4f}\")\n",
    "\n",
    "#Generalization check.\n",
    "if final_model_choice in validation_results:\n",
    "    val_acc = val_results['accuracy']\n",
    "    print(f\"\\n{'='*25}\")\n",
    "    print(f\"Generalization Gap (Validation → Test): {test_acc - val_acc:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize ROC curves for all models from validation phase.\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, data in roc_data.items():\n",
    "    plt.plot(data['fpr'], data['tpr'], \n",
    "             label=f\"{model_name} (AUC = {data['auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Validation Set', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_validation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC curve saved to 'roc_curves_validation.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1244a23-d190-42ac-be64-b5ca5fc8b26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Cross-Dataset ---\n",
      "  lbp: 59 features\n",
      "  hog: 1764 features\n",
      "  color: 6 features\n",
      "  gabor: 8 features\n",
      "\n",
      "Cross-dataset size: 2000 samples\n",
      "Real: 1000, Fake: 1000\n",
      "PCA applied: 1837 --> 480 components\n",
      "\n",
      "Evaluating on cross-dataset...\n",
      "\n",
      "SVM - Cross-Dataset Results:\n",
      " Inference Time: 11.2367 seconds\n",
      " Accuracy: 0.4215\n",
      " Precision: 0.4574\n",
      " Recall: 0.8430\n",
      " F1 Score: 0.5930\n",
      " ROC-AUC: 0.0765\n",
      " Confusion Matrix:\n",
      "   [[   0 1000]\n",
      " [ 157  843]]\n",
      "\n",
      " CONFUSION MATRIX BREAKDOWN:\n",
      "  True Negatives  (TN):    0 - Correctly identified as Fake\n",
      "  False Positives (FP): 1000 - Fake wrongly identified as Real\n",
      "  False Negatives (FN):  157 - Real wrongly identified as Fake\n",
      "  True Positives  (TP):  843 - Correctly identified as Real\n",
      "\n",
      " ERROR ANALYSIS:\n",
      "  Total Errors: 1157/2000 (57.85%)\n",
      "  False Positives: 1000 (50.00%) - Fake images classified as Real\n",
      "  False Negatives: 157 (7.85%) - Real images classified as Fake\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.076518)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the final model on a new dataset.\n",
    "print(\"\\n--- Loading Cross-Dataset ---\")\n",
    "#Load new dataset features.\n",
    "real_cross = np.load('test_cross_real_all_features.npz')\n",
    "fake_cross = np.load('test_cross_fake_all_features.npz')\n",
    "\n",
    "#Extract and combine features for each type.\n",
    "X_cross_dict = {}\n",
    "for feature in features_to_use:\n",
    "    real_feat = real_cross[feature]\n",
    "    fake_feat = fake_cross[feature]\n",
    "    X_cross_dict[feature] = np.vstack([real_feat, fake_feat])\n",
    "    print(f\"  {feature}: {X_cross_dict[feature].shape[1]} features\")\n",
    "    \n",
    "# Create labels (Real=1, Fake=0 to match training convention)\n",
    "y_cross = np.hstack([np.ones(len(real_cross[features_to_use[0]])),   #Real images = 1.\n",
    "                     np.zeros(len(fake_cross[features_to_use[0]]))])  #Fake images = 0.\n",
    "print(f\"\\nCross-dataset size: {len(y_cross)} samples\")\n",
    "print(f\"Real: {np.sum(y_cross == 1)}, Fake: {np.sum(y_cross == 0)}\")\n",
    "\n",
    "#Scale each feature individually using the saved scalers.\n",
    "X_cross_scaled_dict = {}\n",
    "for feature in features_to_use:\n",
    "    X_cross_scaled_dict[feature] = scalers[feature].transform(X_cross_dict[feature])\n",
    "X_cross_scaled = np.concatenate([X_cross_scaled_dict[f] for f in features_to_use], axis=1)\n",
    "if use_pca:\n",
    "    X_cross_final = pca.transform(X_cross_scaled)\n",
    "    print(f\"PCA applied: {X_cross_scaled.shape[1]} --> {X_cross_final.shape[1]} components\")\n",
    "else:\n",
    "    X_cross_final = X_cross_scaled\n",
    "# Get predictions with inference time\n",
    "print(\"\\nEvaluating on cross-dataset...\")\n",
    "cross_infer_start = time.time()\n",
    "y_cross_pred = final_model.predict(X_cross_final)\n",
    "cross_infer_time = time.time() - cross_infer_start\n",
    "# Get probabilities\n",
    "if hasattr(final_model, \"predict_proba\"):\n",
    "    y_cross_proba = final_model.predict_proba(X_cross_final)[:, 1]\n",
    "elif hasattr(final_model, \"decision_function\"):\n",
    "    y_cross_proba = final_model.decision_function(X_cross_final)\n",
    "else:\n",
    "    y_cross_proba = None\n",
    "print_metrics(y_cross, y_cross_pred, y_cross_proba, \"Cross-Dataset\", final_model_choice, cross_infer_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7d231-31a2-4c7f-b251-64ac2cb51af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
