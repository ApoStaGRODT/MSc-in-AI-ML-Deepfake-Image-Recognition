{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe66a015-09eb-41cc-95ee-1f9b6bd8112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,  precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "929a9986-adf4-4005-a7d0-2c8d4a1092d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = ['lbp', 'hog', 'color', 'gabor'] #Added all features for baseline\n",
    "use_pca = True #This way we can enable/disable PCA.\n",
    "pca_variance_threshold = 0.95 #Keep components explaining 95% of variance.\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=1),\n",
    "    'SVM': SVC(random_state=1),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f322bd7-8fd4-42c5-b11b-62b84f882c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions help set a loop/pipeline for experimenting with different features/models.\n",
    "def load_and_combine_features(real_path, fake_path, features_to_use):\n",
    "    #We load real and fake features, combine them with labels.\n",
    "    real_data = np.load(real_path)\n",
    "    fake_data = np.load(fake_path)\n",
    "\n",
    "    #Print individual feature dimensions.\n",
    "    print(f\"  Feature dimensions in {real_path}:\")\n",
    "    for feature in features_to_use:\n",
    "        print(f\"    {feature}: {real_data[feature].shape[1]} features\")\n",
    "    \n",
    "    #We extract and concatenate selected features.\n",
    "    real_features = np.concatenate([real_data[feature] for feature in features_to_use], axis=1)\n",
    "    fake_features = np.concatenate([fake_data[feature] for feature in features_to_use], axis=1)\n",
    "\n",
    "    #We combine real and fake images.\n",
    "    X = np.vstack([real_features, fake_features])\n",
    "    y = np.hstack([np.zeros(len(real_features)), np.ones(len(fake_features))])\n",
    "    return X, y\n",
    "\n",
    "def print_metrics(y_true, y_pred, dataset_name, model_name):\n",
    "    #We choose the following evaluation metrics.\n",
    "    Acc = accuracy_score(y_true, y_pred)\n",
    "    F1 = f1_score(y_true, y_pred)\n",
    "    Precision = precision_score(y_true, y_pred)\n",
    "    Recall = recall_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Extract confusion matrix values\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(f\"\\n{model_name} - {dataset_name} Results:\")\n",
    "    print(f\" Accuracy: {Acc:.4f}\")\n",
    "    print(f\" Precision: {Precision:.4f}\")\n",
    "    print(f\" Recall: {Recall:.4f}\")\n",
    "    print(f\" F1 Score: {F1:.4f}\")\n",
    "    print(f\" Confusion Matrix:\")\n",
    "    print(f\"   {cm}\")\n",
    "\n",
    "    print(f\"\\n CONFUSION MATRIX BREAKDOWN:\")\n",
    "    print(f\"  True Negatives  (TN): {tn:4d} - Correctly identified as Fake\")\n",
    "    print(f\"  False Positives (FP): {fp:4d} - Fake wrongly identified as Real\")\n",
    "    print(f\"  False Negatives (FN): {fn:4d} - Real wrongly identified as Fake\")\n",
    "    print(f\"  True Positives  (TP): {tp:4d} - Correctly identified as Real\")\n",
    "    \n",
    "    # Error analysis\n",
    "    total_errors = fp + fn\n",
    "    total_samples = len(y_true)\n",
    "    print(f\"\\n ERROR ANALYSIS:\")\n",
    "    print(f\"  Total Errors: {total_errors}/{total_samples} ({total_errors/total_samples*100:.2f}%)\")\n",
    "    print(f\"  False Positives: {fp} ({fp/total_samples*100:.2f}%) - Fake images classified as Real\")\n",
    "    print(f\"  False Negatives: {fn} ({fn/total_samples*100:.2f}%) - Real images classified as Fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90c54c57-b6b3-4d53-871e-1c1caf352003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Train ---\n",
      "  Feature dimensions in train_real_all_features.npz:\n",
      "    lbp: 59 features\n",
      "    hog: 1764 features\n",
      "    color: 6 features\n",
      "    gabor: 8 features\n",
      "\n",
      "--- Validation ---\n",
      "  Feature dimensions in valid_real_all_features.npz:\n",
      "    lbp: 59 features\n",
      "    hog: 1764 features\n",
      "    color: 6 features\n",
      "    gabor: 8 features\n",
      "\n",
      "--- Test ---\n",
      "  Feature dimensions in test_real_all_features.npz:\n",
      "    lbp: 59 features\n",
      "    hog: 1764 features\n",
      "    color: 6 features\n",
      "    gabor: 8 features\n",
      "\n",
      "Train set: (10000, 1837), Valid set: (2000, 1837), Test set: (2000, 1837)\n",
      "Total features used: 1837\n",
      "\n",
      "--- Class Balance ---\n",
      "Train - Real: 5000, Fake: 5000\n",
      "Valid - Real: 1000, Fake: 1000\n",
      "Test  - Real: 1000, Fake: 1000\n"
     ]
    }
   ],
   "source": [
    "#Time to load the data.\n",
    "print(\"\\n--- Train ---\")\n",
    "X_train, y_train = load_and_combine_features(\n",
    "    'train_real_all_features.npz',\n",
    "    'train_fake_all_features.npz',\n",
    "    features_to_use\n",
    ")\n",
    "print(\"\\n--- Validation ---\")\n",
    "X_valid, y_valid = load_and_combine_features(\n",
    "    'valid_real_all_features.npz',\n",
    "    'valid_fake_all_features.npz',\n",
    "    features_to_use\n",
    ")\n",
    "print(\"\\n--- Test ---\")\n",
    "X_test, y_test = load_and_combine_features(\n",
    "    'test_real_all_features.npz',\n",
    "    'test_fake_all_features.npz',\n",
    "    features_to_use\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}, Valid set: {X_valid.shape}, Test set: {X_test.shape}\")\n",
    "print(f\"Total features used: {X_train.shape[1]}\")\n",
    "\n",
    "print(\"\\n--- Class Balance ---\")\n",
    "print(f\"Train - Real: {np.sum(y_train == 0)}, Fake: {np.sum(y_train == 1)}\")\n",
    "print(f\"Valid - Real: {np.sum(y_valid == 0)}, Fake: {np.sum(y_valid == 1)}\")\n",
    "print(f\"Test  - Real: {np.sum(y_test == 0)}, Fake: {np.sum(y_test == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f34d4b5-5725-4eb5-811d-e0b18acc9d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaling features...\n",
      "Scaler saved to 'scaler.joblib'\n"
     ]
    }
   ],
   "source": [
    "#Scaling all features (before PCA).\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Save scaler\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"Scaler saved to 'scaler.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bad2ddfb-65c4-4520-b731-b80ec03f1d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA Results:\n",
      " Original features: 1837\n",
      " Components retained: 473\n",
      " Explained variance: 0.9502 (95.02%)\n",
      " Dimensionality reduction: 1837 --> 473\n",
      "PCA saved to 'pca.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#Apply PCA\n",
    "if use_pca:\n",
    "    #We fit PCA on training data only.\n",
    "    pca = PCA(n_components=pca_variance_threshold, random_state=1)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "    #We transform validation and test using the fitted PCA.\n",
    "    X_valid_pca = pca.transform(X_valid_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    n_components = pca.n_components_\n",
    "    explained_var = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"\\nPCA Results:\")\n",
    "    print(f\" Original features: {X_train_scaled.shape[1]}\")\n",
    "    print(f\" Components retained: {n_components}\")\n",
    "    print(f\" Explained variance: {explained_var:.4f} ({explained_var*100:.2f}%)\")\n",
    "    print(f\" Dimensionality reduction: {X_train_scaled.shape[1]} --> {n_components}\")\n",
    "\n",
    "    joblib.dump(pca, 'pca.joblib')\n",
    "    print(\"PCA saved to 'pca.joblib\")\n",
    "\n",
    "    #Use PCA-transformed data for training.\n",
    "    X_train_scaled = X_train_pca\n",
    "    X_valid_scaled = X_valid_pca\n",
    "    X_test_scaled = X_test_pca\n",
    "\n",
    "else:\n",
    "    print(\"\\nPCA Disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f181380-1ba9-4cc3-bb02-a2db131eda76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Training Random Forest ...\n",
      "\n",
      "Random Forest - Train Results:\n",
      " Accuracy: 1.0000\n",
      " F1 Score: 1.0000\n",
      " Confusion Matrix:\n",
      "   [[5000    0]\n",
      " [   0 5000]]\n",
      "\n",
      "Random Forest - Valid Results:\n",
      " Accuracy: 0.7385\n",
      " F1 Score: 0.7308\n",
      " Confusion Matrix:\n",
      "   [[767 233]\n",
      " [290 710]]\n",
      "\n",
      "Random Forest - Test Results:\n",
      " Accuracy: 0.7370\n",
      " F1 Score: 0.7330\n",
      " Confusion Matrix:\n",
      "   [[752 248]\n",
      " [278 722]]\n",
      "\n",
      "Model saved to 'random_forest_model.joblib'\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Training Gradient Boosting ...\n",
      "\n",
      "Gradient Boosting - Train Results:\n",
      " Accuracy: 0.8355\n",
      " F1 Score: 0.8373\n",
      " Confusion Matrix:\n",
      "   [[4123  877]\n",
      " [ 768 4232]]\n",
      "\n",
      "Gradient Boosting - Valid Results:\n",
      " Accuracy: 0.7310\n",
      " F1 Score: 0.7388\n",
      " Confusion Matrix:\n",
      "   [[701 299]\n",
      " [239 761]]\n",
      "\n",
      "Gradient Boosting - Test Results:\n",
      " Accuracy: 0.7400\n",
      " F1 Score: 0.7502\n",
      " Confusion Matrix:\n",
      "   [[699 301]\n",
      " [219 781]]\n",
      "\n",
      "Model saved to 'gradient_boosting_model.joblib'\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Training SVM ...\n",
      "\n",
      "SVM - Train Results:\n",
      " Accuracy: 0.9591\n",
      " F1 Score: 0.9591\n",
      " Confusion Matrix:\n",
      "   [[4794  206]\n",
      " [ 203 4797]]\n",
      "\n",
      "SVM - Valid Results:\n",
      " Accuracy: 0.8300\n",
      " F1 Score: 0.8322\n",
      " Confusion Matrix:\n",
      "   [[817 183]\n",
      " [157 843]]\n",
      "\n",
      "SVM - Test Results:\n",
      " Accuracy: 0.8310\n",
      " F1 Score: 0.8327\n",
      " Confusion Matrix:\n",
      "   [[821 179]\n",
      " [159 841]]\n",
      "\n",
      "Model saved to 'svm_model.joblib'\n",
      "\n",
      "==================================================\n",
      "Baseline: All models trained and saved with all features (lbp, hog, color, gabor)\n"
     ]
    }
   ],
   "source": [
    "#Time to train and evaluate all models.\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'+'*50}\")\n",
    "    print(f\"Training {model_name} ...\")\n",
    "\n",
    "    #Training.\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    #Predicting on all sets.\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_valid_pred = model.predict(X_valid_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    #Printing metrics.\n",
    "    print_metrics(y_train, y_train_pred, \"Train\", model_name)\n",
    "    print_metrics(y_valid, y_valid_pred, \"Valid\", model_name)\n",
    "    print_metrics(y_test, y_test_pred, \"Test\", model_name)\n",
    "\n",
    "    #Saving models.\n",
    "    model_filename = f\"{model_name.lower().replace(' ', '_')}_model.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"\\nModel saved to '{model_filename}'\")\n",
    "\n",
    "    #Storing in dictionary.\n",
    "    trained_models[model_name] = model\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Baseline: All models trained and saved with all features (lbp, hog, color, gabor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7812797-f2c0-40c0-9b6c-98a156bf01e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
